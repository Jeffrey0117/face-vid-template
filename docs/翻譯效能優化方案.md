# 翻譯工作流程效能優化方案

## 現況問題

| 瓶頸 | 說明 | 影響 |
|------|------|------|
| Whisper CPU | 原版 Whisper 在 CPU 上極慢 | 65 秒影片需 5-30 分鐘 |
| 逐條翻譯 | 781 個片段 = 781 次 API 呼叫 | 翻譯耗時 30+ 分鐘 |
| 無進度顯示 | 使用者不知道進度 | 體驗差 |

---

## 優化方案

### 1. Whisper 加速：faster-whisper

**效能提升：4-8 倍**

| 方案 | 速度 | 記憶體 | 安裝難度 |
|------|------|--------|----------|
| 原版 whisper (CPU) | 1x | 高 | 簡單 |
| **faster-whisper (CPU)** | **4x** | **低 50%** | 簡單 |
| faster-whisper (GPU) | **20-40x** | 低 | 需 CUDA |

#### 安裝
```bash
pip install faster-whisper
```

#### 使用方式
```python
from faster_whisper import WhisperModel

# CPU 模式
model = WhisperModel("base", device="cpu", compute_type="int8")

# GPU 模式 (需要 NVIDIA CUDA)
model = WhisperModel("base", device="cuda", compute_type="float16")

# 轉錄
segments, info = model.transcribe("video.mp4", language="en")
for segment in segments:
    print(f"[{segment.start:.2f}s -> {segment.end:.2f}s] {segment.text}")
```

#### 批次處理 (更快)
```python
from faster_whisper import BatchedInferencePipeline, WhisperModel

model = WhisperModel("base", device="cuda", compute_type="float16")
batched_model = BatchedInferencePipeline(model)

segments, info = batched_model.transcribe(
    "video.mp4",
    batch_size=16,  # 批次大小
    language="en"
)
```

---

### 2. 翻譯加速：批次翻譯

**效能提升：10-20 倍**

**問題**：781 個片段 = 781 次 API 呼叫 = 30+ 分鐘

**解法**：將多個片段合併成一次 API 呼叫

#### 優化前 (逐條)
```python
for segment in segments:  # 781 次迴圈
    translated = translate(segment.text)  # 781 次 API 呼叫
```

#### 優化後 (批次)
```python
# 每 20 個片段合併成一次請求
batch_size = 20
for i in range(0, len(segments), batch_size):
    batch = segments[i:i+batch_size]
    texts = [f"[{j}] {s.text}" for j, s in enumerate(batch)]
    combined = "\n".join(texts)

    # 一次翻譯 20 條
    response = client.chat.completions.create(
        model="deepseek-chat",
        messages=[
            {"role": "system", "content": "翻譯以下編號字幕，保持編號格式"},
            {"role": "user", "content": combined}
        ]
    )
    # 解析回應，分割回各片段
```

**效果**：781 次 → 40 次 API 呼叫

---

### 3. GPU 加速 (可選)

如果有 NVIDIA 顯卡：

```bash
# 安裝 CUDA 版 PyTorch
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121

# 安裝 faster-whisper GPU 依賴
pip install nvidia-cublas-cu12 nvidia-cudnn-cu12
```

**效能對比**：

| 配置 | 65 秒影片轉錄時間 |
|------|------------------|
| whisper base (CPU) | 2-5 分鐘 |
| faster-whisper base (CPU) | 30-60 秒 |
| faster-whisper base (GPU) | **5-10 秒** |

---

## 實作優先順序

### Phase 1：立即可做 (無需 GPU)
1. ✅ 替換 whisper → faster-whisper
2. ✅ 實作批次翻譯 (20 條/次)
3. ✅ 加入即時進度顯示

**預期效果**：總時間從 40 分鐘 → 5-10 分鐘

### Phase 2：有 GPU 時
1. 安裝 CUDA 版 PyTorch
2. 啟用 GPU 加速
3. 使用 BatchedInferencePipeline

**預期效果**：總時間 → 1-2 分鐘

---

## 其他備選方案

| 方案 | 說明 | 適用場景 |
|------|------|----------|
| whisper.cpp | C++ 實作，極快 | Windows 獨立執行檔 |
| WhisperX | 更精準的時間戳 | 需要說話者分離時 |
| Groq API | 雲端 Whisper，超快 | 願意付費、不介意上傳 |
| AssemblyAI | 雲端 ASR 服務 | 商業用途 |

---

## 建議配置

### 最佳 CP 值配置 (無 GPU)
```json
{
  "whisper": {
    "library": "faster-whisper",
    "model": "base",
    "device": "cpu",
    "compute_type": "int8"
  },
  "translation": {
    "batch_size": 20,
    "model": "deepseek-chat"
  }
}
```

### 最快配置 (有 GPU)
```json
{
  "whisper": {
    "library": "faster-whisper",
    "model": "medium",
    "device": "cuda",
    "compute_type": "float16",
    "batch_size": 16
  },
  "translation": {
    "batch_size": 30,
    "model": "deepseek-chat"
  }
}
```

---

*文件建立：2024-12-26*
